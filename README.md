This is IRIS team solution for LLMs4OL 2025 Challenge. This is an initial commit which is missing requirements (to be added soon) 

## ğŸ“ Repository Layout & Key Scripts

This repository contains the **IRIS team** solution for the **LLMs4OL 2025 Challenge**.  
All task-specific code lives under `LLMs4OL/src/`, organised by sub-task (B, C, D) and ontology
(MatOnto, OBI, SWEET).

LLMs4OL_2025/
â”œâ”€â”€ LLMs4OL/
â”‚ â””â”€â”€ src/
â”‚ â”œâ”€â”€ TaskB/ # Term-typing (multi-label classification)
â”‚ â”‚ â”œâ”€â”€ matonto/
â”‚ â”‚ â”‚ â”œâ”€ augment_data_matonto.py # â‡¢ generates surface-form variants & caches synonyms
â”‚ â”‚ â”‚ â”œâ”€ collect_matonto_term_defs.py # â‡¢ scrapes/generates short type definitions
â”‚ â”‚ â”‚ â”œâ”€ train_inference_matonto_configurable.py
â”‚ â”‚ â”‚ â”‚ â€¢ toggles data-augmentation / definitions
â”‚ â”‚ â”‚ â”‚ â€¢ trains BERT/DeBERTa + threshold tuning (micro-F1)
â”‚ â”‚ â”‚ â””â”€ *term_definitions.json / train_augmented.jsonl
â”‚ â”‚ â”œâ”€â”€ obi/ â€¦same trio of scripts/resources for OBI
â”‚ â”‚ â””â”€â”€ sweet/ â€¦same trio + train_deberta_sweet_cls.py
â”‚ â”‚
â”‚ â”œâ”€â”€ TaskC/ # Taxonomy discovery (parentâ€“child edges)
â”‚ â”‚ â”œâ”€â”€ matonto/
â”‚ â”‚ â”‚ â”œâ”€ fetch_matonto_defs.py # gathers extra definitions
â”‚ â”‚ â”‚ â”œâ”€ filter_candidates_matonto.py # FAISS + SBERT semantic filtering
â”‚ â”‚ â”‚ â””â”€ matonto_train_and_infer_configurable.py
â”‚ â”‚ â”œâ”€â”€ obi/ â€¦analogous scripts
â”‚ â”‚ â””â”€â”€ sweet/ â€¦analogous scripts
â”‚ â”‚
â”‚ â””â”€â”€ TaskD/
â”‚ â””â”€â”€ sweet/ # Non-taxonomic relation extraction
â”‚ â”œâ”€ build_sweetD_candidates_semantic_only.py
â”‚ â”œâ”€ sweet_RE_train_and_infer_configurable.py
â”‚ â””â”€ candidates_sweetD{train,test}.json
â”‚
â”œâ”€â”€ LICENSE (Apache-2.0)
â””â”€â”€ README.md


### ğŸ”‘ Script Cheat-Sheet

| Script | What it does | Output |
|--------|--------------|--------|
| **augment_data_\*** | Augments training terms with unit symbols, plural/-case variants, Wiktionary & GPT-4o â‰¤3-word synonyms (all cached). | `train_augmented.jsonl` |
| **collect_\*term_defs.py** | Mines compact English definitions (LLM + heuristics) for each ontology type. | `*_term_definitions.json` |
| **train_inference_\*_configurable.py** | End-to-end pipeline<br>â–ª build stratified splits â†’ tokenize â†’ train BERT/DeBERTa multi-label classifier<br>â–ª dev-set threshold sweep to maximise micro-F1<br>â–ª writes submission JSON. | model checkpoints, `preds_*.json` |
| **filter_candidates_\*** | Embeds candidate pairs in SBERT + FAISS IP index, prunes to top-K semantic matches. | `candidates_*.json` |
| **\*train_and_infer_configurable.py** (Task C/D) | Fine-tunes a classification/ranking model on filtered candidates; ties into same threshold-sweep logic for predictions. | model checkpoints, submission JSON |

> **Note:** Raw LLMs4OL 2025 datasets are **not** committed. Place the original task folders under `LLMs4OL/2025/` before running any script.

---

Feel free to tell me if you want a longer, tutorial-style README (installation, GPU requirements, run commands, citation) or a diagram of the end-to-end pipelineâ€”happy to flesh it out!
::contentReference[oaicite:0]{index=0}
