This is IRIS team solution for LLMs4OL 2025 Challenge. This is an initial commit which is missing requirements (to be added soon) 

## ğŸ“ Repository Layout & Key Scripts

This repository contains the **IRIS team** solution for the **LLMs4OL 2025 Challenge**.  
All task-specific code lives under `LLMs4OL/src/`, organised by sub-task (B, C, D) and ontology
(MatOnto, OBI, SWEET).

<code>
LLMs4OL_2025/
â”œâ”€â”€ LLMs4OL/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ TaskB/                      # Term-typing (multi-label classification)
â”‚       â”‚   â”œâ”€â”€ matonto/
â”‚       â”‚   â”‚   â”œâ”€â”€ augment_data_matonto.py           # surface-form variants & synonym cache
â”‚       â”‚   â”‚   â”œâ”€â”€ collect_matonto_term_defs.py      # scrape / generate type definitions
â”‚       â”‚   â”‚   â”œâ”€â”€ train_inference_matonto_configurable.py
â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ toggles data-augmentation / definitions
â”‚       â”‚   â”‚   â”‚   â””â”€â”€ trains BERT/DeBERTa + threshold tuning (micro-F1)
â”‚       â”‚   â”‚   â””â”€â”€ *term_definitions.json  /  train_augmented.jsonl
â”‚       â”‚   â”œâ”€â”€ obi/                     # same trio for OBI
â”‚       â”‚   â””â”€â”€ sweet/                   # same trio + train_deberta_sweet_cls.py
â”‚       â”‚
â”‚       â”œâ”€â”€ TaskC/                      # Taxonomy discovery (parentâ€“child edges)
â”‚       â”‚   â”œâ”€â”€ matonto/
â”‚       â”‚   â”‚   â”œâ”€â”€ fetch_matonto_defs.py            # gather extra definitions
â”‚       â”‚   â”‚   â”œâ”€â”€ filter_candidates_matonto.py     # FAISS + SBERT semantic filtering
â”‚       â”‚   â”‚   â””â”€â”€ matonto_train_and_infer_configurable.py
â”‚       â”‚   â”œâ”€â”€ obi/                     # analogous scripts
â”‚       â”‚   â””â”€â”€ sweet/                   # analogous scripts
â”‚       â”‚
â”‚       â””â”€â”€ TaskD/
â”‚           â””â”€â”€ sweet/                   # Non-taxonomic relation extraction
â”‚               â”œâ”€â”€ build_sweetD_candidates_semantic_only.py
â”‚               â”œâ”€â”€ sweet_RE_train_and_infer_configurable.py
â”‚               â””â”€â”€ candidates_sweetD_{train,test}.json
â”‚
â”œâ”€â”€ LICENSE            # Apache-2.0
â””â”€â”€ README.md
</code>

### ğŸ”‘ Script Cheat-Sheet

| Script | What it does | Output |
|--------|--------------|--------|
| **augment_data_\*** | Augments training terms with unit symbols, plural/-case variants, Wiktionary & GPT-4o â‰¤3-word synonyms (all cached). | `train_augmented.jsonl` |
| **collect_\*term_defs.py** | Mines compact English definitions (LLM + heuristics) for each ontology type. | `*_term_definitions.json` |
| **train_inference_\*_configurable.py** | End-to-end pipeline<br>â–ª build stratified splits â†’ tokenize â†’ train BERT/DeBERTa multi-label classifier<br>â–ª dev-set threshold sweep to maximise micro-F1<br>â–ª writes submission JSON. | model checkpoints, `preds_*.json` |
| **filter_candidates_\*** | Embeds candidate pairs in SBERT + FAISS IP index, prunes to top-K semantic matches. | `candidates_*.json` |
| **\*train_and_infer_configurable.py** (Task C/D) | Fine-tunes a classification/ranking model on filtered candidates; ties into same threshold-sweep logic for predictions. | model checkpoints, submission JSON |

> **Note:** Raw LLMs4OL 2025 datasets are **not** committed. Place the original task folders under `LLMs4OL/2025/` before running any script.

---

Feel free to tell me if you want a longer, tutorial-style README (installation, GPU requirements, run commands, citation) or a diagram of the end-to-end pipelineâ€”happy to flesh it out!
::contentReference[oaicite:0]{index=0}
