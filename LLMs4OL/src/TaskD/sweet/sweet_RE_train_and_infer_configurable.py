# ───────────────────────────────────────────────────────────────────────────
# Configurable binary RE classifier for SWEET (Task-D).
#   • USE_DEFS  : include definitions in prompt?
#   • NEG_SRC   : "random"  |  "candidates"
#     - negatives generated by relation/head/tail corruption
#     - corrupted head/tail drawn either randomly or from neighbour list
# Saves tuned τ to best_threshold.txt and writes JSON edges.
# ───────────────────────────────────────────────────────────────────────────

# ╭─ variant switches ─────────────────────────────────────────────────────╮
USE_DEFS  = True                 # True→definitions, False→raw strings
NEG_SRC   = "candidates"         # "random" | "candidates"
NEG_PER_POS = 9 if NEG_SRC=="candidates" else 9
# ╰──────────────────────────────────────────────────────────────────────────╯
# ───────── imports (torch after CUDA env) ─────────────────────────────────
import os, json, random, math, numpy as np
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
import torch, torch.nn as nn

from pathlib import Path
from collections import Counter
from tqdm import tqdm
from datasets import Dataset
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    DataCollatorWithPadding, TrainingArguments, Trainer)
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
from scipy.special import softmax

# ───────── PATHS ──────────────────────────────────────────────────────────
ROOT = Path("LLMs4OL")
# To run the script add original data files (2025) to LLMs4OL folder
TRAIN_TRIPLE = ROOT / "2025/TaskD-NonTaxonomicRE/SWEET/train/sweet_train_re_pairs.json"
TRAIN_TYPES  = ROOT / "2025/TaskD-NonTaxonomicRE/SWEET/train/sweet_train_re_types.txt"
TRAIN_RELS   = ROOT / "2025/TaskD-NonTaxonomicRE/SWEET/train/sweet_train_re_relations.txt"

TEST_TYPES   = ROOT / "2025/TaskD-NonTaxonomicRE/SWEET/test/sweet_test_re_types.txt"
TEST_RELS    = ROOT / "2025/TaskD-NonTaxonomicRE/SWEET/test/sweet_test_re_relations.txt"
#Types here are reused from subtask C
DEFS_JSON    = ROOT / "src/TaskC/sweet_binary/sweet_types_leads.json"
CAND_TRAIN   = ROOT / "src/TaskD/sweet_binary/candidates_sweetD_train.json"
CAND_TEST    = ROOT / "src/TaskD/sweet_binary/candidates_sweetD_test.json"

TAG      = f"sweetRE_{'def' if USE_DEFS else 'str'}_{NEG_SRC}"
OUT_DIR  = ROOT / f"src/TaskD/sweet_binary/ckpt_deberta_v3_{TAG}"
PRED_JS  = ROOT / f"src/TaskD/sweet_binary/pred_triples_{TAG}.json"
MODEL_ID = "microsoft/deberta-v3-large"

# ───────── HYPERPARAMS ────────────────────────────────────────────────────
SEED, BS, EPOCHS, LR = 42, 2, 6, 5e-6
MAX_LEN   = 512 if USE_DEFS else 256
DEV_FR, TEST_FR = 0.10, 0.10
INF_BATCH = 32

random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ───────── LOAD DATA ──────────────────────────────────────────────────────
triples_all = json.loads(TRAIN_TRIPLE.read_text("utf-8"))
train_types = [t.strip() for t in TRAIN_TYPES.read_text("utf-8").splitlines() if t.strip()]
train_rels  = [r.strip() for r in TRAIN_RELS.read_text("utf-8").splitlines() if r.strip()]

test_types  = [t.strip() for t in TEST_TYPES.read_text("utf-8").splitlines() if t.strip()]
test_rels   = [r.strip() for r in TEST_RELS.read_text("utf-8").splitlines() if r.strip()]

defs_map    = json.loads(DEFS_JSON.read_text("utf-8")) if USE_DEFS else {}
cand_train  = json.loads(CAND_TRAIN.read_text("utf-8")) if NEG_SRC=="candidates" else {}
cand_test   = json.loads(CAND_TEST.read_text("utf-8"))  if NEG_SRC=="candidates" else {}

ALL_TYPES   = train_types                     # only train types used for random tails

# symmetric relations (extend list if needed)
SYMM = {"equivalentClass", "synonymOf"}

# ───────── SPLIT train/dev/test ───────────────────────────────────────────
random.shuffle(triples_all)
n_tot = len(triples_all); n_dev = math.floor(0.10*n_tot); n_tst = math.floor(0.10*n_tot)
dev_triples   = triples_all[:n_dev]
test_triples  = triples_all[n_dev:n_dev+n_tst]
train_triples = triples_all[n_dev+n_tst:]
gold_set      = {(tr["head"], tr["relation"], tr["tail"]) for tr in train_triples}

print(f"[INFO] triples train/dev/test = {len(train_triples)}/{len(dev_triples)}/{len(test_triples)}")

# ───────── PROMPT & CORRUPTION HELPERS ────────────────────────────────────
def prompt(h, r, t):
    if USE_DEFS:
        return (f"[HEAD] {h}\n[DEF] {defs_map.get(h,'')}\n\n"
                f"[RELATION] {r}\n\n"
                f"[TAIL] {t}\n[DEF] {defs_map.get(t,'')}")
    return f"[HEAD] {h}\n\n[RELATION] {r}\n\n[TAIL] {t}"

def corrupt_triple(h, r, t, mode):
    if mode == "rel":
        r_neg = random.choice([x for x in train_rels if x != r])
        return h, r_neg, t
    pool = cand_train.get(h if mode=="tail" else t, []) if NEG_SRC=="candidates" else ALL_TYPES
    if not pool:                           # safety fallback
        pool = ALL_TYPES
    if mode == "head":
        return random.choice(pool), r, t
    else:
        return h, r, random.choice(pool)

# ───────── BUILD EXAMPLES ─────────────────────────────────────────────────
def make_examples(triples, tag):
    examples = []
    for tr in triples:
        h, r, t = tr["head"], tr["relation"], tr["tail"]
        directions = [(h, t)] + ([(t, h)] if r in SYMM else [])
        for hp, tp in directions:
            examples.append({"text": prompt(hp, r, tp), "label": 1})
            gen = 0
            while gen < NEG_PER_POS:
                mode = random.choice(["rel", "head", "tail"])
                cand = corrupt_triple(hp, r, tp, mode)
                if cand in gold_set:
                    continue
                examples.append({"text": prompt(*cand), "label": 0})
                gen += 1
    print(f"[{tag}] examples = {len(examples):,}")
    return examples

train_data = make_examples(train_triples, "train")
dev_data   = make_examples(dev_triples,   "dev")
test_data  = make_examples(test_triples,  "test")

# ───────── TOKENISATION ───────────────────────────────────────────────────
tok = AutoTokenizer.from_pretrained(MODEL_ID)
def enc(b): return tok(b["text"], truncation=True, max_length=MAX_LEN)
train_ds = Dataset.from_list(train_data).map(enc, batched=True, remove_columns=["text"])
dev_ds   = Dataset.from_list(dev_data  ).map(enc, batched=True, remove_columns=["text"])
test_ds  = Dataset.from_list(test_data ).map(enc, batched=True, remove_columns=["text"])
collator = DataCollatorWithPadding(tok)

# ───────── MODEL & TRAINER ────────────────────────────────────────────────
model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, num_labels=2).to(device)
cnt = Counter(e["label"] for e in train_data)
pos_w = (cnt[0] / cnt[1]) * 0.4

class WTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **_):
        labels = inputs.pop("labels")
        outs   = model(**inputs)
        loss   = nn.CrossEntropyLoss(weight=torch.tensor([1.0, pos_w], device=outs.logits.device))(outs.logits, labels)
        if return_outputs:
            detached = {k:(v.detach() if isinstance(v, torch.Tensor) else v) for k, v in outs.items()}
            return loss, outs.__class__(**detached)
        return loss
def metrics(pred):
    logits, labels = pred
    preds = logits.argmax(-1)
    p, r, f, _ = precision_recall_fscore_support(labels, preds, average="binary", zero_division=0)
    return {"accuracy": accuracy_score(labels, preds), "precision": p, "recall": r, "f1": f}

args = TrainingArguments(
    output_dir=str(OUT_DIR),
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=LR,
    per_device_train_batch_size=BS,
    per_device_eval_batch_size=BS,
    num_train_epochs=EPOCHS,
    fp16=True,
    seed=SEED,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    report_to="none",
    save_total_limit=2,
)
trainer = WTrainer(model=model, args=args,
                   train_dataset=train_ds, eval_dataset=dev_ds,
                   tokenizer=tok, data_collator=collator,
                   compute_metrics=metrics)
trainer.train()

model.save_pretrained(OUT_DIR)
tok.save_pretrained(OUT_DIR)

# ───────── tune τ on dev & save ───────────────────────────────────────────
probs_dev = softmax(trainer.predict(dev_ds).predictions, axis=-1)[:, 1]
tau, _ = max(
    ((thr, precision_recall_fscore_support(dev_ds["label"], (probs_dev >= thr).astype(int),
                                           average="binary", zero_division=0)[2])
     for thr in np.arange(0.05, 0.951, 0.01)),
    key=lambda x: x[1]
)
(OUT_DIR / "best_threshold.txt").write_text(f"{tau:.4f}\n", "utf-8")
print(f"[DEV] best τ = {tau:.2f}   saved to best_threshold.txt")

# ───────── internal test metrics ──────────────────────────────────────────
probs_test = softmax(trainer.predict(test_ds).predictions, axis=-1)[:, 1]
test_pred  = (probs_test >= tau).astype(int)
p, r, f, _ = precision_recall_fscore_support(test_ds["label"], test_pred, average="binary", zero_division=0)
print(f"[TEST internal]  F1={f:.3f}  P={p:.3f}  R={r:.3f}")

# ───────── OFFICIAL TEST INFERENCE ────────────────────────────────────────
print("[INFO] inference on official SWEET RE test triples …")

pred, texts, triples = [], [], []

def flush():
    if not texts:
        return
    with torch.no_grad():
        probs = softmax(
            model(**tok(texts, padding=True, truncation=True, max_length=MAX_LEN,
                        return_tensors="pt").to(device)
                  ).logits.detach().cpu().numpy(), axis=-1)[:, 1]
    for (h, r, t), s in zip(triples, probs):
        if s >= tau:
            pred.append({"head": h, "relation": r, "tail": t, "score": float(s)})
    texts.clear(); triples.clear()

for head in tqdm(test_types, unit="head"):
    tail_pool = cand_test.get(head, []) if NEG_SRC == "candidates" else \
                random.sample(train_types, min(NEG_PER_POS*5, len(train_types)))
    if not tail_pool:           # shouldn't happen, but guard
        tail_pool = train_types
    for rel in test_rels:
        for tail in tail_pool:
            if tail == head:
                continue
            triples.append((head, rel, tail))
            texts.append(prompt(head, rel, tail))
            if len(texts) >= INF_BATCH:
                flush()
flush()

PRED_JS.parent.mkdir(parents=True, exist_ok=True)
PRED_JS.write_text(json.dumps(pred, indent=2, ensure_ascii=False), "utf-8")
print(f"[DONE] {len(pred):,} triples saved → {PRED_JS}")